#Metric for CLassification models

# Importing necessary functions from scikit-learn metrics module
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Sample actual and predicted labels for a binary classification task
actual_labels = [0, 1, 1, 0, 1, 0, 1, 0, 1, 1]
predicted_labels = [0, 1, 0, 1, 1, 0, 1, 0, 0, 1]

# Computing classification metrics

# Accuracy: Ratio of correctly predicted observations to the total observations.
accuracy = accuracy_score(actual_labels, predicted_labels)

# Precision: Ratio of true positives to the sum of true positives and false positives.
precision = precision_score(actual_labels, predicted_labels)

# Recall: Ratio of true positives to the sum of true positives and false negatives.
recall = recall_score(actual_labels, predicted_labels)

# F1-Score: Harmonic mean of precision and recall, balances false positives and false negatives.
f1 = f1_score(actual_labels, predicted_labels)

# Confusion Matrix: Table showing true positives, true negatives, false positives, and false negatives.
conf_matrix = confusion_matrix(actual_labels, predicted_labels)

# Displaying the computed metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-Score:", f1)
print("Confusion Matrix:\n", conf_matrix)
